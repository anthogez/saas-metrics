# Logstash Pipeline Configuration
# Version: 1.0.0
# Dependencies:
# - logstash-input-beats@7.17.0
# - logstash-filter-json@3.2.0 
# - logstash-filter-grok@4.4.0
# - logstash-output-elasticsearch@11.0.0

input {
  beats {
    port => "${BEATS_PORT:5044}"
    ssl => true
    ssl_certificate => "${SSL_CERTIFICATE_PATH:/etc/logstash/ssl/logstash.crt}"
    ssl_key => "${SSL_KEY_PATH:/etc/logstash/ssl/logstash.key}"
    ssl_verify_mode => "force_peer"
    include_codec_tag => true
    client_inactivity_timeout => 60
  }
}

filter {
  # Split processing based on log type
  if [fields][log_type] == "application" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] \[%{DATA:service}\] \[%{DATA:trace_id}\] %{GREEDYDATA:message}(?:\n%{GREEDYDATA:stack_trace})?"
      }
      overwrite => ["message"]
      tag_on_failure => ["_grokparsefailure", "application_parse_error"]
      add_field => {
        "log_source" => "application"
        "environment" => "%{[fields][env]}"
      }
    }

    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      timezone => "UTC"
    }

    if [level] == "ERROR" {
      mutate {
        add_field => { "alert_required" => true }
      }
    }

    # Parse JSON if present in message
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "structured_data"
        skip_on_invalid_json => true
        tag_on_failure => ["json_parse_error"]
      }
    }
  }
  
  else if [fields][log_type] == "audit" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[AUDIT\] user=%{DATA:user_id} action=%{WORD:action} resource=%{DATA:resource} changes=%{GREEDYDATA:changes}"
      }
      tag_on_failure => ["_grokparsefailure", "audit_parse_error"]
      add_field => {
        "log_source" => "audit"
        "retention_years" => "3"
      }
    }

    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      timezone => "UTC"
    }

    # Parse changes JSON
    json {
      source => "changes"
      target => "change_details"
      skip_on_invalid_json => true
      tag_on_failure => ["audit_json_parse_error"]
    }

    # Encrypt sensitive fields
    ruby {
      code => '
        sensitive_fields = ["user_id", "changes"]
        event.to_hash.each do |key, value|
          if sensitive_fields.include?(key)
            event.set("#{key}_encrypted", value.to_s.encrypt)
          end
        end
      '
    }
  }

  # Common processing for all logs
  mutate {
    remove_field => ["message", "beat", "input", "prospector", "offset"]
    add_field => {
      "environment" => "${ENV:production}"
      "application" => "saas-metrics"
    }
  }
}

output {
  elasticsearch {
    hosts => ["${ES_HOST:elasticsearch:9200}"]
    user => "${ES_USER:elastic}"
    password => "${ES_PASSWORD:changeme}"
    ssl => true
    ssl_certificate_verification => true
    
    # Dynamic index naming based on log type
    index => "%{[fields][log_type]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
    
    # Index lifecycle management
    ilm_enabled => true
    ilm_rollover_alias => "${INDEX_PREFIX:saas-metrics}-%{[fields][log_type]}"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "%{[fields][log_type]}_policy"
    
    # Retry configuration
    retry_on_conflict => 3
    action => "index"
    
    # Document management
    document_id => "%{[@metadata][_id]}"
    manage_template => true
    template_overwrite => true
    
    # Error handling
    failure_type_logging_whitelist => ["es_rejected_execution_exception"]
    sniffing => true
    validate_after_inactivity => 2000
  }

  # Dead letter queue for failed events
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ES_HOST:elasticsearch:9200}"]
      index => "logstash-dead-letter-%{+YYYY.MM.dd}"
      user => "${ES_USER:elastic}"
      password => "${ES_PASSWORD:changeme}"
    }
  }
}